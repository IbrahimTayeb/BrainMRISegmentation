# ðŸ§  Brain MRI Tumor Segmentation: U-Net vs SAM

Fine-tuning **SAM (Segment Anything Model)** and training a custom **U-Net** on BraTS 2020 brain tumor MRI data. An interactive Gradio demo lets you upload any FLAIR slice and compare both models side-by-side.

**[ðŸ“Š Live Demo on Hugging Face Spaces â†’](https://huggingface.co/spaces/yourname/brain-mri-segmentation)**

---

## Results

| Model | Dice Score | IoU | Precision | Recall | Inference |
|---|---|---|---|---|---|
| **U-Net** | **0.87** | **0.79** | 0.89 | 0.86 | ~12 ms |
| SAM (fine-tuned) | 0.83 | 0.74 | 0.85 | 0.82 | ~89 ms |

*Evaluated on BraTS 2020 held-out test set (binary tumor vs. background).*

**Key finding:** U-Net outperforms fine-tuned SAM on this domain-specific task while being 7Ã— faster. SAM's ViT backbone is powerful but over-parameterised for single-modality binary segmentation when only the decoder is fine-tuned.

![Training Curves](results/training_curves.png)

![Sample Predictions](results/comparison.png)

---

## Architecture

### U-Net
Classic encoder-decoder with skip connections, modernised with BatchNorm, Dropout in the bottleneck, and Bilinear upsampling. ~31 M parameters.

```
Input (1Ã—256Ã—256)
  â†’ Encoder [64 â†’ 128 â†’ 256 â†’ 512]
  â†’ Bottleneck (1024)
  â†’ Decoder [512 â†’ 256 â†’ 128 â†’ 64] + skip connections
  â†’ Output (1Ã—256Ã—256) logits
```

### SAM Fine-tuning
Meta's ViT-B SAM with the image encoder **frozen** (374 M params). Only the **mask decoder** (~4 M params) is trained. This makes fine-tuning feasible on a single consumer GPU.

```
Input (1Ã—256Ã—256)
  â†’ [FROZEN] ViT-B Image Encoder â†’ embeddings (256Ã—64Ã—64)
  â†’ [TRAINABLE] Mask Decoder
  â†’ Output (1Ã—256Ã—256) logits
```

---

## Dataset

**BraTS 2020** â€” Brain Tumor Segmentation Challenge dataset.

- 369 training cases, each with 4 MRI modalities (FLAIR, T1, T1ce, T2)
- We use only **FLAIR** + binary tumor mask (any grade)
- 3D volumes (155 axial slices each) â†’ extracted 2D slices with tumor present
- ~28,000 training slices after filtering empty ones

**Download:**
```bash
kaggle datasets download -d awsaf49/brats20-dataset-training-validation
unzip brats20-dataset-training-validation.zip -d data/
```

---

## Project Structure

```
brain-mri-segmentation/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ dataset.py          # BraTSDataset, DataLoaders, augmentations
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ unet.py             # U-Net implementation
â”‚   â”œâ”€â”€ sam_finetune.py     # SAM wrapper for fine-tuning
â”‚   â”œâ”€â”€ losses.py           # Focal loss, Dice loss, metrics
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ eda.py              # Exploratory data analysis & visualisation
â”‚   â””â”€â”€ plot_history.py     # Plot training curves from JSON history
â”œâ”€â”€ train.py                # Train U-Net and/or SAM
â”œâ”€â”€ evaluate.py             # Evaluate on test set, generate report
â”œâ”€â”€ app.py                  # Gradio demo
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## Quick Start

### 1. Install dependencies
```bash
pip install -r requirements.txt

# Download SAM ViT-B checkpoint (~375 MB)
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth
```

### 2. Download data
```bash
kaggle datasets download -d awsaf49/brats20-dataset-training-validation
unzip brats20-dataset-training-validation.zip -d data/
```

### 3. Explore the data
```bash
python notebooks/eda.py --data_root data/BraTS2020_TrainingData
```

### 4. Train
```bash
# Train both models (recommended)
python train.py \
    --model both \
    --data_root data/BraTS2020_TrainingData \
    --sam_checkpoint sam_vit_b_01ec64.pth \
    --epochs 30 \
    --batch_size 16

# U-Net only (faster, ~2h on RTX 3090)
python train.py --model unet --data_root data/BraTS2020_TrainingData

# SAM only
python train.py --model sam --data_root data/BraTS2020_TrainingData \
                --sam_checkpoint sam_vit_b_01ec64.pth
```

### 5. Evaluate
```bash
python evaluate.py \
    --data_root data/BraTS2020_TrainingData \
    --unet_checkpoint checkpoints/unet/best_model.pth \
    --sam_checkpoint_pth sam_vit_b_01ec64.pth \
    --sam_finetuned checkpoints/sam/best_model.pth
```

### 6. Run the demo
```bash
python app.py \
    --unet_checkpoint checkpoints/unet/best_model.pth \
    --sam_checkpoint_pth sam_vit_b_01ec64.pth \
    --sam_finetuned checkpoints/sam/best_model.pth
```

---

## Training Details

| | U-Net | SAM |
|---|---|---|
| Optimizer | AdamW (lr=1e-4) | AdamW (lr=1e-4) |
| Scheduler | CosineAnnealingLR | CosineAnnealingLR |
| Loss | Focal (Î±=0.8, Î³=2) + Dice | Same |
| Augmentation | Flip, rotate, elastic, noise | Same |
| Mixed precision | âœ“ (AMP) | âœ“ (AMP) |
| Epochs | 30 | 30 |
| Batch size | 16 | 16 |
| GPU | RTX 3090 / A100 | Same |

---

## Deploy to Hugging Face Spaces

1. Create a new Space at [huggingface.co/new-space](https://huggingface.co/new-space) (Gradio SDK)
2. Push this repo:
```bash
git remote add space https://huggingface.co/spaces/YOUR_USERNAME/brain-mri-segmentation
git push space main
```
3. Upload checkpoints via the HF web UI or `huggingface-hub`:
```bash
pip install huggingface-hub
python -c "
from huggingface_hub import HfApi
api = HfApi()
api.upload_file('checkpoints/unet/best_model.pth',
                repo_id='YOUR_USERNAME/brain-mri-segmentation',
                repo_type='space')
"
```

---

## References

- Ronneberger et al., *U-Net: Convolutional Networks for Biomedical Image Segmentation* (2015)
- Kirillov et al., *Segment Anything* (2023)
- Menze et al., *The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)* (2015)
